{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking with the help of claude\n",
    "\n",
    "### *Please note that this is a low level implementation of chunking based of llms* there is a way to fine tune a llm specifically for chunking but may require more resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Rahul.G\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('E:\\\\Projects\\\\SA - R&D\\\\chunking')\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\",category=UserWarning)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from utils.pdfExt import main\n",
    "from pypdf import PdfReader\n",
    "from fuzzywuzzy import fuzz\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import anthropic\n",
    "import textwrap\n",
    "import requests\n",
    "import openai\n",
    "import nltk\n",
    "import json\n",
    "import time \n",
    "import re\n",
    "import io\n",
    "import os\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(message:str,success_flag=True):\n",
    "    if success_flag: print(f\"\\n\\n###################   {message}   ###################\")\n",
    "    else: print(f\"!!!!!!!!!!!!!!!!!!   {message}   !!!!!!!!!!!!!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracts both text and images from pdfs \n",
    "extract_data() -> function can take both local path and urls for pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(pdf_path_or_url : str, output_folder=r'./data/img') -> str:\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # If PDF is a URL, download it\n",
    "    if pdf_path_or_url.startswith(\"http\"):\n",
    "        response = requests.get(pdf_path_or_url)\n",
    "        log(\"Downloading the pdf.\")\n",
    "        if response.status_code == 200:\n",
    "            pdf_data = response.content\n",
    "        else:\n",
    "            log(f\"Failed to download PDF from {pdf_path_or_url}\",True)\n",
    "            return 404\n",
    "    else:\n",
    "    \n",
    "        with open(pdf_path_or_url, 'rb') as f:\n",
    "            pdf_data = f.read()\n",
    "\n",
    "    reader = PdfReader(io.BytesIO(pdf_data))\n",
    "    text = ''.join([page.extract_text() for page in reader.pages])\n",
    "    wrapped_text = textwrap.fill(text, width=120)\n",
    "    \n",
    "    for page_num, page in enumerate(reader.pages, start=1):\n",
    "        for i, image in enumerate(page.images, start=1):\n",
    "            image_data = io.BytesIO(image.data)\n",
    "            try:\n",
    "                img = Image.open(image_data)\n",
    "                image_name = f\"page{page_num}_img{i}\"\n",
    "                image_path = os.path.join(output_folder, f\"{image_name}.{img.format.lower()}\")\n",
    "                img.save(image_path)\n",
    "                log(f\"Image extracted: {image_path}\")\n",
    "            except Exception as e:\n",
    "                log(f\"Failed to extract image: {e}\",True)\n",
    "                           \n",
    "    # print(\"\\n\\n\")\n",
    "    \n",
    "    return wrapped_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sometimes the extracted text from the pdfs are sooo bad so I added a post processing function to format uneven text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(raw_text):\n",
    "    \n",
    "    formatted_text = ' '.join(raw_text.split())\n",
    "    formatted_text = ''.join(char if char.isalnum() or char.isspace() else ' ' for char in formatted_text)\n",
    "    sections = formatted_text.split('   ')\n",
    "    formatted_text = ''\n",
    "    for section in sections:\n",
    "        if section.strip():\n",
    "            formatted_text += '   ' + section.strip() + '\\n\\n'\n",
    "\n",
    "    return formatted_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using claud sonnet for faster response time (balanced with response time and intelligence) \n",
    "\n",
    "max_tokens is 4096 woking on ways to sove this issue ,finding a solution to give more tokens to sonnet\n",
    "\n",
    "other available models : *Claude 3 Haiku*(faster respone with low intelligence),*Claude 3 Sonnet*(balanced),*Claude 3 Opus*(slower response time with higher intelligence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call LLM\n",
    "def generate_raw_chunks(user_prompt:str):\n",
    "\n",
    "    client = OpenAI(\n",
    "        api_key=os.getenv('GPT_KEY')\n",
    "    )\n",
    "    \n",
    "    system_prompt = \"\"\"Given the provided text data, your task is to chunk the text into meaningful segments or 'chunks' based on the topics or sections mentioned within the text. Each chunk should encapsulate a distinct topic or subtopic discussed within the text corpus. Your goal is to parse the text into coherent units that represent the main themes or ideas conveyed in the text.\n",
    "\n",
    "    You can identify the boundaries of each chunk by looking for section headers or topic labels within the text. These headers typically indicate the start of a new topic or section. Your output should consist of the identified chunks, along with their corresponding labels or headers.\n",
    "\n",
    "    Please ensure that each chunk is clearly delineated and captures a cohesive set of information related to its respective topic or theme. Additionally, consider the overall structure and coherence of the chunks to facilitate understanding and interpretation by readers.\n",
    "\n",
    "    Feel free to leverage the contextual information provided in the text to guide your chunking process. Remember, the objective is to organize the text into digestible segments that effectively convey the main ideas discussed within the text corpus.\n",
    "\n",
    "    <important>Note: You should not modify the text in the corpus; your only job is to split (chunk) the corpus accordingly. your are strictly not allowed to reduce the content of chunk it should be same as the raw corpse provides. if the input corpse is 1000 tokents the output should also be 1000 tokens,if the input corpse is 2000 tokens the output tokents should be 2000.if a chunk croses 800 words please divide it if a chunk is 1600 words divide it by 800 woord chunk and 800 word chunk. \n",
    "    \n",
    "    The chunks should follow a format like this:\n",
    "\n",
    "    <chunk 1>\n",
    "    Topic:topic for chunk 1\n",
    "    Content:content of Chunk 1\n",
    "    </chunk 1>\n",
    "    ...\n",
    "    \n",
    "    Remember : you should not reduce content nor summarise it your only job is to divide corpse to chunks. the chunks should be a perfect sub-class of corpse(super-class).\n",
    "    </important>\n",
    "\n",
    "    \"\"\"\n",
    "    log(\"Genrating raw chunks\")\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            model=os.getenv('GPT_MODEL_NAME'),\n",
    "            max_tokens=4096,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": f\"Here is the corpse\\n <important> You are strictly not allowed to modify this corpse your only job is to split this corpse into chunks(that makes sense)</important>\\n<corpse>\\n {user_prompt} \\n</corpse>\"}\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        return chat_completion.choices[0].message.content\n",
    "    \n",
    "    except openai.APIConnectionError as e:\n",
    "        warnings.warn(\"Network Error Retry Later\",category=RuntimeWarning)\n",
    "        # sys.exit(-1)\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###################   Genrating raw chunks   ###################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Rahul.G\\AppData\\Local\\Temp\\ipykernel_12376\\4262354890.py:44: RuntimeWarning: Network Error Retry Later\n",
      "  warnings.warn(\"Network Error Retry Later\",category=RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "generate_raw_chunks(\"who are you\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function is used to process output of claude to a specific format \n",
    "\n",
    "Note : calculation of start and end indexes are not so accuarte cuz claude sometimes modifies chunks content so finding start and end indexes of chunks is a challenge\n",
    "\n",
    "trying different approaches to increase the accuracy of finding start and end indexes\n",
    "\n",
    "reason for not using claude to format output for us is to maximize the accuracy of getting good chunks ,tried claude to format results, but claude ended up hallucinating and the chunk quality become so poor (sent 3600 tokens to chunk but got only 1000 - 1500 tokens back from claude) but without making claude to format data for us,we got upto 3500 tokens back out of 3600."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_corpse(text):\n",
    "    max_tokens = 3900\n",
    "    min_tokens = 3600\n",
    "    paragraph_separator = '\\n\\n'\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    total_tokens = len(tokens)\n",
    "\n",
    "    if total_tokens <= max_tokens:\n",
    "        return [text]  # If the total number of tokens is within the range, return the original string as a single segment\n",
    "    \n",
    "    segments = []\n",
    "    current_segment = []\n",
    "    token_count = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        token_count += 1  # Increment token count for each token\n",
    "        current_segment.append(token)\n",
    "\n",
    "        if token_count >= min_tokens and (token_count >= max_tokens or token == paragraph_separator):\n",
    "            # If the token count reaches the minimum required, and either exceeds the maximum or a paragraph separator is found,\n",
    "            # add the current segment to the segments list\n",
    "            segments.append(' '.join(current_segment))\n",
    "            current_segment = []\n",
    "            token_count = 0\n",
    "    \n",
    "    # Add the last segment if there are any remaining tokens\n",
    "    if current_segment:\n",
    "        segments.append(' '.join(current_segment))\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(corpus, raw_chunks, save_flag, display_flag,is_folder,is_fresh):\n",
    "    \n",
    "    def convert_to_json(raw_data):\n",
    "\n",
    "        topic_pattern = re.compile(r'Topic: (.+)')\n",
    "        subtopic_pattern = re.compile(r'Subtopic: (.+)')\n",
    "        content_pattern = re.compile(r'Content:\\s*(.*?)\\s*(?=\\n<chunk \\d+>|$)', re.DOTALL)\n",
    "        formatted_chunks = []\n",
    "        chunks = raw_data.split('<chunk')\n",
    "\n",
    "        for chunk in chunks[1:]:\n",
    "            formatted_chunk = {}\n",
    "            \n",
    "            topic_match = topic_pattern.search(chunk)\n",
    "            if topic_match:\n",
    "                formatted_chunk['topic'] = topic_match.group(1).strip()\n",
    "\n",
    "            subtopic_match = subtopic_pattern.search(chunk)\n",
    "            if subtopic_match:\n",
    "                formatted_chunk['subtopic'] = subtopic_match.group(1).strip()\n",
    "\n",
    "            content_match = content_pattern.search(chunk)\n",
    "            if content_match:\n",
    "                content = content_match.group(1).strip()\n",
    "                content = re.sub(r'\\n</chunk \\d+>$', '', content)\n",
    "                formatted_chunk['content'] = content\n",
    "\n",
    "            formatted_chunks.append(formatted_chunk)\n",
    "            \n",
    "        if is_folder:\n",
    "            with open(r'results\\raw_chunks.json', 'a') as json_file:\n",
    "                json.dump(formatted_chunks, json_file, indent=2)\n",
    "        else:\n",
    "            with open(r'results\\raw_chunks.json', 'w') as json_file:\n",
    "                json.dump(formatted_chunks, json_file, indent=2)\n",
    "            \n",
    "        return formatted_chunks\n",
    "    \n",
    "    pre_form_json = convert_to_json(raw_chunks)\n",
    "    \n",
    "    contents = [content[\"content\"] for content in pre_form_json]\n",
    "    topics = [topic[\"topic\"] for topic in pre_form_json]\n",
    "    subtopics = [subtopic.get(\"subtopic\", None) for subtopic in pre_form_json]\n",
    "    \n",
    "    output = []\n",
    "    start_index = 0\n",
    "    total_tokens = 0\n",
    "    for idx, content in enumerate(contents):\n",
    "        topic = topics[idx]\n",
    "        subtopic = subtopics[idx]\n",
    "       \n",
    "        tokens = word_tokenize(content)\n",
    "        \n",
    "        # Check if content exceeds 800 tokens\n",
    "        if len(tokens) > 800:\n",
    "            # Split content into smaller chunks\n",
    "            num_chunks = len(tokens) // 800 + 1\n",
    "            chunk_size = len(tokens) // num_chunks\n",
    "            token_chunks = [tokens[i:i+chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "            chunked_content = [' '.join(chunk) for chunk in token_chunks]\n",
    "        else:\n",
    "            chunked_content = [content]\n",
    "        total_tokens += len(tokens)\n",
    "        for chunk_content in chunked_content:\n",
    "            # ignore this for now finding better ways to find indexes\n",
    "            match = fuzz.partial_ratio(corpus, chunk_content)\n",
    "            \n",
    "            end_index = min(len(corpus), start_index + len(chunk_content))\n",
    "            \n",
    "            if subtopic != None:\n",
    "                output.append({\n",
    "                \"title\": topic,\n",
    "                \"subtopic\":subtopic,\n",
    "                \"content\": chunk_content,\n",
    "                \"start_index\": start_index,\n",
    "                \"end_index\": end_index,\n",
    "                \"num_tokens\":len(tokens)\n",
    "            }) \n",
    "                \n",
    "            else:\n",
    "                output.append({\n",
    "                \"title\": topic,\n",
    "                \"content\": chunk_content,\n",
    "                \"start_index\": start_index,\n",
    "                \"end_index\": end_index,\n",
    "                \"num_tokens\":len(tokens)\n",
    "                })\n",
    "            \n",
    "            \n",
    "            start_index = end_index + 1\n",
    "    \n",
    "    if save_flag:\n",
    "        file_path = r'results\\chunks.json'\n",
    "\n",
    "        if is_folder:\n",
    "            if is_fresh:\n",
    "                existing_data = []\n",
    "                is_fresh = False\n",
    "                \n",
    "            else:\n",
    "                with open(file_path, 'r') as file:\n",
    "                    existing_data = json.load(file)\n",
    "            \n",
    "            existing_data.extend(output)\n",
    "\n",
    "            with open(file_path, 'w') as json_file:\n",
    "                json.dump(existing_data, json_file, indent=2)\n",
    "                \n",
    "            \n",
    "        else:\n",
    "            with open(file_path, 'w') as json_file:\n",
    "                json.dump(output, json_file, indent=2)\n",
    "            log(r\"please Take a look at results\\chunks.json for chunks\")\n",
    "            \n",
    "            return\n",
    "\n",
    "        log(r\"please Take a look at results\\chunks.json for chunks\")\n",
    "\n",
    "            \n",
    "            \n",
    "    else:\n",
    "        warnings.warn(\"Note : Chunks are not saved \\n Reason : save_flag - False \",category=Warning)\n",
    "            \n",
    "    if display_flag:\n",
    "        print(json.dumps(output, indent=2))\n",
    "        \n",
    "    return is_fresh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Process:\n",
    "\n",
    "###     1. Extract text and images from the given pdf\n",
    "###     2. Post process text (images are ignored for now, but can process images)\n",
    "###     3. Sent the extracted text to claude to chunk based on contex\n",
    "###     4. Post processed claude's response to desired format (each chunk does not exceed 800 tokens wrote a logic for it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_single_pdf(timer : bool,pdf_path : str,display_flag = False,save_flag  = True):\n",
    "    \n",
    "    \n",
    "    if timer:\n",
    "        start_time = time.time()\n",
    "    \n",
    "    \n",
    "    log(\"Called PDF extracter\")\n",
    "    try:\n",
    "        corpus = main(pdf_filepath=pdf_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(\"cant open file\")\n",
    "        sys.exit(-1)\n",
    "        \n",
    "    log(\"Extracted PDF data\")\n",
    "    # print(corpus)\n",
    "    corpus = format_text(corpus)\n",
    "    # print(\"\\n\\n\")\n",
    "    print(corpus)\n",
    "    \n",
    "    result = split_corpse(corpus)\n",
    "    raw_chunk = ''\n",
    "    for segment in result:\n",
    "        raw_chunk_ = generate_raw_chunks(user_prompt=segment)\n",
    "        raw_chunk += \"\\n\\n\" + raw_chunk_\n",
    "        \n",
    "    # log(\"Raw Chunks\")\n",
    "    # print(raw_chunk)\n",
    "    \n",
    "    log(\"Post Processing chunks\")\n",
    "    \n",
    "    pre_process(corpus=corpus,raw_chunks=raw_chunk,save_flag=save_flag,display_flag=display_flag,is_folder=False)\n",
    "    \n",
    "    if timer:\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate the total time taken\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        log(f\" Total time taken to run: {total_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_multiple_pdf(timer : bool,folder_path : str,display_flag = False,save_flag  = True):\n",
    "    \n",
    "    if timer:\n",
    "        start_time = time.time()\n",
    "    \n",
    "    is_fresh = True\n",
    "    log(\"Called PDF extracter\")\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file is a PDF\n",
    "        if filename.endswith('.pdf'):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            try:\n",
    "                corpus = main(pdf_filepath=pdf_path)\n",
    "            except FileNotFoundError as e:\n",
    "                print(\"cant open file\")\n",
    "                sys.exit(-1)\n",
    "        \n",
    "            log(f\"Extracted PDF data for {filename}\")\n",
    "            corpus = format_text(corpus)\n",
    "            # print(\"\\n\\n\")\n",
    "            print(corpus)\n",
    "            \n",
    "            result = split_corpse(corpus)\n",
    "            raw_chunk = ''\n",
    "            for segment in result:\n",
    "                raw_chunk_ = generate_raw_chunks(user_prompt=segment)\n",
    "                raw_chunk += \"\\n\\n\" + raw_chunk_\n",
    "                \n",
    "            log(\"Raw Chunks\")\n",
    "            print(raw_chunk)\n",
    "            \n",
    "            log(f\"Post Processing raw chunks for {filename}\")\n",
    "            \n",
    "            is_fresh = pre_process(corpus=corpus,raw_chunks=raw_chunk,save_flag=save_flag,display_flag=display_flag,is_folder=True,is_fresh=is_fresh)\n",
    "            log(f\"Chunked {filename} succesfully\")\n",
    "            \n",
    "    if timer:\n",
    "        end_time = time.time()\n",
    "        \n",
    "        # Calculate the total time taken\n",
    "        total_time = end_time - start_time\n",
    "        \n",
    "        log(f\" Total time taken to run: {total_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    os.environ['GPT_KEY'] = \"Your api key here\"\n",
    "    os.environ['GPT_MODEL_NAME'] = \"gpt-4o\"\n",
    "\n",
    "    chunk_multiple_pdf(True,r\"Your Folder Path here\",False,True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
