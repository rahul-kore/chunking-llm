{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking with the help of claude\n",
    "\n",
    "### *Please note that this is a low level implementation of chunking based of llms* there is a way to fine tune a llm specifically for chunking but may require more resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('E:\\\\Projects\\\\SA - R&D\\\\chunking\\\\resources')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pypdf import PdfReader\n",
    "from fuzzywuzzy import fuzz\n",
    "from PIL import Image\n",
    "import nltk\n",
    "import anthropic\n",
    "import textwrap\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import io\n",
    "import os\n",
    "from pdfExt import main\n",
    "# from custom_pdf_parser import main\n",
    "\n",
    "# from GenerativeAIExamples.RetrievalAugmentedGeneration.examples.multimodal_rag.vectorstore.custom_pdf_parser import main\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(message:str,success_flag=True):\n",
    "    if success_flag: print(f\"\\n\\n###################   {message}   ###################\")\n",
    "    else: print(f\"!!!!!!!!!!!!!!!!!!   {message}   !!!!!!!!!!!!!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracts both text and images from pdfs \n",
    "extract_data() -> function can take both local path and urls for pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_data(pdf_path_or_url : str, output_folder=r'./data/img') -> str:\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    \n",
    "    # If PDF is a URL, download it\n",
    "    if pdf_path_or_url.startswith(\"http\"):\n",
    "        response = requests.get(pdf_path_or_url)\n",
    "        log(\"Downloading the pdf.\")\n",
    "        if response.status_code == 200:\n",
    "            pdf_data = response.content\n",
    "        else:\n",
    "            log(f\"Failed to download PDF from {pdf_path_or_url}\",True)\n",
    "            return 404\n",
    "    else:\n",
    "    \n",
    "        with open(pdf_path_or_url, 'rb') as f:\n",
    "            pdf_data = f.read()\n",
    "\n",
    "    reader = PdfReader(io.BytesIO(pdf_data))\n",
    "    text = ''.join([page.extract_text() for page in reader.pages])\n",
    "    wrapped_text = textwrap.fill(text, width=120)\n",
    "    \n",
    "    for page_num, page in enumerate(reader.pages, start=1):\n",
    "        for i, image in enumerate(page.images, start=1):\n",
    "            image_data = io.BytesIO(image.data)\n",
    "            try:\n",
    "                img = Image.open(image_data)\n",
    "                image_name = f\"page{page_num}_img{i}\"\n",
    "                image_path = os.path.join(output_folder, f\"{image_name}.{img.format.lower()}\")\n",
    "                img.save(image_path)\n",
    "                log(f\"Image extracted: {image_path}\")\n",
    "            except Exception as e:\n",
    "                log(f\"Failed to extract image: {e}\",True)\n",
    "                           \n",
    "    # print(\"\\n\\n\")\n",
    "    \n",
    "    return wrapped_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sometimes the extracted text from the pdfs are sooo bad so I added a post processing function to format uneven text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text(raw_text):\n",
    "    \n",
    "    formatted_text = ' '.join(raw_text.split())\n",
    "    formatted_text = ''.join(char if char.isalnum() or char.isspace() else ' ' for char in formatted_text)\n",
    "    sections = formatted_text.split('   ')\n",
    "    formatted_text = ''\n",
    "    for section in sections:\n",
    "        if section.strip():\n",
    "            formatted_text += '   ' + section.strip() + '\\n\\n'\n",
    "\n",
    "    return formatted_text.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using claud sonnet for faster response time (balanced with response time and intelligence) \n",
    "\n",
    "max_tokens is 4096 woking on ways to sove this issue ,finding a solution to give more tokens to sonnet\n",
    "\n",
    "other available models : *Claude 3 Haiku*(faster respone with low intelligence),*Claude 3 Sonnet*(balanced),*Claude 3 Opus*(slower response time with higher intelligence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call LLM\n",
    "def generate_raw_chunks(user_prompt:str)->str:\n",
    "    \n",
    "    client = anthropic.Client(api_key=\"Your api key here\")\n",
    "\n",
    "    system_prompt = \"\"\"Given the provided text data, your task is to chunk the text into meaningful segments or 'chunks' based on the topics or sections mentioned within the text. Each chunk should encapsulate a distinct topic or subtopic discussed within the text corpus. Your goal is to parse the text into coherent units that represent the main themes or ideas conveyed in the text.\n",
    "\n",
    "    You can identify the boundaries of each chunk by looking for section headers or topic labels within the text. These headers typically indicate the start of a new topic or section. Your output should consist of the identified chunks, along with their corresponding labels or headers.\n",
    "\n",
    "    Please ensure that each chunk is clearly delineated and captures a cohesive set of information related to its respective topic or theme. Additionally, consider the overall structure and coherence of the chunks to facilitate understanding and interpretation by readers.\n",
    "\n",
    "    Feel free to leverage the contextual information provided in the text to guide your chunking process. Remember, the objective is to organize the text into digestible segments that effectively convey the main ideas discussed within the text corpus.\n",
    "\n",
    "    <important>Note: You should not modify the text in the corpus; your only job is to split (chunk) the corpus accordingly. your are strictly not allowed to reduce the content of chunk it should be same as the raw corpse provides. if the input corpse is 1000 tokents the output should also be 1000 tokens,if the input corpse is 2000 tokens the output tokents should be 2000.if a chunk croses 800 words please divide it if a chunk is 1600 words divide it by 800 woord chunk and 800 word chunk. \n",
    "    \n",
    "    The chunks should follow a format like this:\n",
    "\n",
    "    <chunk 1>\n",
    "    Topic:topic for chunk 1\n",
    "    Content:content of Chunk 1\n",
    "    </chunk 1>\n",
    "    ...\n",
    "    \n",
    "    Remember : you should not reduce content nor summarise it your only job is to divide corpse to chunks. the chunks should be a perfect sub-class of corpse(super-class).\n",
    "    </important>\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    log(\"Genrating raw chunks\")\n",
    "    \n",
    "    response = client.messages.create(\n",
    "        model=\"claude-3-opus-20240229\",\n",
    "        max_tokens=4096,\n",
    "        system=system_prompt,\n",
    "        messages=\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": f\"Here is the corpse\\n <important> You are strictly not allowed to modify this corpse your only job is to split this corpse into chunks(that makes sense)</important>\\n<corpse>\\n {user_prompt} \\n</corpse>\"}\n",
    "        ]\n",
    "    )\n",
    "    print(\"response\" , response)\n",
    "    return response.content[0].text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This function is used to process output of claude to a specific format \n",
    "\n",
    "Note : calculation of start and end indexes are not so accuarte cuz claude sometimes modifies chunks content so finding start and end indexes of chunks is a challenge\n",
    "\n",
    "trying different approaches to increase the accuracy of finding start and end indexes\n",
    "\n",
    "reason for not using claude to format output for us is to maximize the accuracy of getting good chunks ,tried claude to format results, but claude ended up hallucinating and the chunk quality become so poor (sent 3600 tokens to chunk but got only 1000 - 1500 tokens back from claude) but without making claude to format data for us,we got upto 3500 tokens back out of 3600."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(corpus, raw_chunks, test_flag, save_flag, display_flag):\n",
    "    \n",
    "    def convert_to_json(raw_data):\n",
    "\n",
    "        topic_pattern = re.compile(r'Topic: (.+)')\n",
    "        subtopic_pattern = re.compile(r'Subtopic: (.+)')\n",
    "        content_pattern = re.compile(r'Content:\\s*(.*?)\\s*(?=\\n<chunk \\d+>|$)', re.DOTALL)\n",
    "        formatted_chunks = []\n",
    "        chunks = raw_data.split('<chunk')\n",
    "\n",
    "        for chunk in chunks[1:]:\n",
    "            formatted_chunk = {}\n",
    "            \n",
    "            topic_match = topic_pattern.search(chunk)\n",
    "            if topic_match:\n",
    "                formatted_chunk['topic'] = topic_match.group(1).strip()\n",
    "\n",
    "            subtopic_match = subtopic_pattern.search(chunk)\n",
    "            if subtopic_match:\n",
    "                formatted_chunk['subtopic'] = subtopic_match.group(1).strip()\n",
    "\n",
    "            content_match = content_pattern.search(chunk)\n",
    "            if content_match:\n",
    "                content = content_match.group(1).strip()\n",
    "                content = re.sub(r'\\n</chunk \\d+>$', '', content)\n",
    "                formatted_chunk['content'] = content\n",
    "\n",
    "            formatted_chunks.append(formatted_chunk)\n",
    "            \n",
    "        # if save_flag:\n",
    "        with open('raw_chunks.json', 'w') as json_file:\n",
    "            json.dump(formatted_chunks, json_file, indent=2)\n",
    "            \n",
    "        return formatted_chunks\n",
    "    \n",
    "    pre_form_json = convert_to_json(raw_chunks)\n",
    "    \n",
    "    contents = [content[\"content\"] for content in pre_form_json]\n",
    "    topics = [topic[\"topic\"] for topic in pre_form_json]\n",
    "    subtopics = [subtopic.get(\"subtopic\", None) for subtopic in pre_form_json]\n",
    "    \n",
    "    output = []\n",
    "    start_index = 0\n",
    "    total_tokens = 0\n",
    "    for idx, content in enumerate(contents):\n",
    "        topic = topics[idx]\n",
    "        subtopic = subtopics[idx]\n",
    "       \n",
    "        tokens = word_tokenize(content)\n",
    "        \n",
    "        # Check if content exceeds 800 tokens\n",
    "        if len(tokens) > 800:\n",
    "            # Split content into smaller chunks\n",
    "            num_chunks = len(tokens) // 800 + 1\n",
    "            chunk_size = len(tokens) // num_chunks\n",
    "            token_chunks = [tokens[i:i+chunk_size] for i in range(0, len(tokens), chunk_size)]\n",
    "            chunked_content = [' '.join(chunk) for chunk in token_chunks]\n",
    "        else:\n",
    "            chunked_content = [content]\n",
    "        total_tokens += len(tokens)\n",
    "        for chunk_content in chunked_content:\n",
    "            # ignore this for now finding better ways to find indexes\n",
    "            match = fuzz.partial_ratio(corpus, chunk_content)\n",
    "            \n",
    "            end_index = min(len(corpus), start_index + len(chunk_content))\n",
    "            \n",
    "            if subtopic != None:\n",
    "                output.append({\n",
    "                \"title\": topic,\n",
    "                \"subtopic\":subtopic,\n",
    "                \"content\": chunk_content,\n",
    "                \"start_index\": start_index,\n",
    "                \"end_index\": end_index,\n",
    "                \"num_tokens\":len(tokens)\n",
    "            }) \n",
    "                \n",
    "            else:\n",
    "                output.append({\n",
    "                \"title\": topic,\n",
    "                \"content\": chunk_content,\n",
    "                \"start_index\": start_index,\n",
    "                \"end_index\": end_index,\n",
    "                \"num_tokens\":len(tokens)\n",
    "                })\n",
    "            \n",
    "            \n",
    "            start_index = end_index + 1\n",
    "    \n",
    "    if save_flag:\n",
    "        with open('chunks.json', 'w') as json_file:\n",
    "            json.dump(output, json_file, indent=2)\n",
    "        log(\"please Take a look at chunks.json for chunks\")\n",
    "    \n",
    "    if display_flag:\n",
    "        print(json.dumps(output, indent=2))\n",
    "        \n",
    "    return total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_corpse(text):\n",
    "    max_tokens = 3900\n",
    "    min_tokens = 3600\n",
    "    paragraph_separator = '\\n\\n'\n",
    "    \n",
    "    tokens = word_tokenize(text)\n",
    "    total_tokens = len(tokens)\n",
    "\n",
    "    if total_tokens <= max_tokens:\n",
    "        return [text]  # If the total number of tokens is within the range, return the original string as a single segment\n",
    "    \n",
    "    segments = []\n",
    "    current_segment = []\n",
    "    token_count = 0\n",
    "\n",
    "    for token in tokens:\n",
    "        token_count += 1  # Increment token count for each token\n",
    "        current_segment.append(token)\n",
    "\n",
    "        if token_count >= min_tokens and (token_count >= max_tokens or token == paragraph_separator):\n",
    "            # If the token count reaches the minimum required, and either exceeds the maximum or a paragraph separator is found,\n",
    "            # add the current segment to the segments list\n",
    "            segments.append(' '.join(current_segment))\n",
    "            current_segment = []\n",
    "            token_count = 0\n",
    "    \n",
    "    # Add the last segment if there are any remaining tokens\n",
    "    if current_segment:\n",
    "        segments.append(' '.join(current_segment))\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunking Process:\n",
    "\n",
    "###     1. Extract text and images from the given pdf\n",
    "###     2. Post process text (images are ignored for now, but can process images)\n",
    "###     3. Sent the extracted text to claude to chunk based on contex\n",
    "###     4. Post processed claude's response to desired format (each chunk does not exceed 800 tokens wrote a logic for it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "###################   Called PDF extracter   ###################\n",
      "\n",
      "\n",
      "###################   Extracted PDF data   ###################\n",
      "Image Description(Skipped)\n",
      "\n",
      "Ordinary Commute Policy \n",
      "\n",
      "February 2023 \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      "What is ordinary commute? .......................................................................................................................... 3 \n",
      "\n",
      "1. \n",
      "What is classed as a permanent workplace? ......................................................................................... 3 \n",
      "\n",
      "2. \n",
      "Roles with one normal workplace .......................................................................................................... 3 \n",
      "\n",
      "3. \n",
      "Roles with two or more normal workplaces, Dual located or multi-sited roles ........................................ 3 \n",
      "\n",
      "4. \n",
      "Regionally based roles .......................................................................................................................... 4 \n",
      "\n",
      "b. \n",
      "You live outside your region ............................................................................................................... 4 \n",
      "\n",
      "5. \n",
      "Nationally based roles ........................................................................................................................... 5 \n",
      "\n",
      "6. \n",
      "Temporary workplace ............................................................................................................................ 5 \n",
      "\n",
      "7. \n",
      "When does a temporary workplace become a permanent workplace? .................................................. 5 \n",
      "\n",
      "8. \n",
      "FAQ’s……………………………………………………………………………………………………………….7 \n",
      "\n",
      "Guide owner ................................................................................................................................................. 6 \n",
      "\n",
      "Ownership and confidentiality ....................................................................................................................... 6 \n",
      "\n",
      "Appendix 1 – Commute type and what you can claim for………………………………………………………….8 \n",
      " \n",
      " \n",
      "\n",
      "\n",
      " \n",
      "\n",
      " \n",
      "\n",
      "The commute is based on how often you visit the same location and annual leave is factored into this.  You may visit differing people \n",
      "at the same location, or you may visit the same location regularly for only a couple of hours, in both scenario’s this would still be \n",
      "classed as an ordinary commute. \n",
      "\n",
      "It is your responsibility to ensure that you comply with the ‘Commute rules’ when completing your normal mileage return.  We will \n",
      "conduct regular audits to ensure that there are no unintended infringements.  Managers are expected to review their teams mileage \n",
      "to ensure compliance. \n",
      "\n",
      "There is also a requirement for both the Manager and colleague to ensure that the correct normal commute locations are entered \n",
      "into SAP. \n",
      "\n",
      " \n",
      " \n",
      "\n",
      "1. What is classed as a permanent workplace? \n",
      "\n",
      "It is usually clear whether or not a location is your permanent workplace (and, therefore, whether a journey to or from that place is \n",
      "ordinary commuting). A place is a permanent workplace if you attend it regularly to perform your duties of employment, and it is not \n",
      "a temporary workplace (see Section 7). \n",
      "\n",
      "You attend a workplace regularly if your attendance is: \n",
      "\n",
      "• \n",
      "Frequent (At least once a week, it does not have to be the same day each week) \n",
      "\n",
      "• \n",
      "For all or almost all of the period for which you hold or are likely to hold that employment \n",
      "\n",
      "It’s possible that you may have two or more normal workplaces depending on your working arrangements. \n",
      "\n",
      "2. Roles with one normal workplace \n",
      "\n",
      "If you have one normal place of work, travel to and from this location is your ordinary commute and cannot be claimed for.  \n",
      " \n",
      "\n",
      "For example: \n",
      "I work in Haydock DC and travel there from home five days a week. I cannot claim any of my journeys to or from Haydock. \n",
      "\n",
      "3. Roles with two or more normal workplaces, Dual located or multi-sited roles \n",
      "\n",
      "If you have more than one normal place of work, travel to and from any of these locations is your ordinary commute and cannot be \n",
      "claimed for or supplemented. \n",
      "\n",
      "For example: \n",
      "I am based in the Equity office, but I travel to Eccles every Monday and Deeside every Wednesday. All three of these \n",
      "locations will be my normal place of work and I cannot claim any of my journeys to or from them. \n",
      "\n",
      "If you have one or more normal place(s) of work and travel to a different location for a temporary business purpose and a limited \n",
      "duration, travel from home to the different location can be claimed for. \n",
      "\n",
      "Being cost conscious, we don’t expect you to claim if your journey is shorter than your ordinary commute and/or it costs less. \n",
      "Effectively you are already saving money on your journey. \n",
      " \n",
      " \n",
      "\n",
      "For example: \n",
      "I am based in the Wellingborough DC, but I attend a training day at the Northampton branch. I can claim all of my journey \n",
      "to and from home and the Northampton branch, as this is longer than my normal commute to Wellingborough. When I \n",
      "attended a training day at Kettering, this journey was much shorter than my normal commute to Wellingborough, therefore \n",
      "I didn’t claim for the journey. \n",
      "\n",
      "I am based in Hatfield DC and I need to attend a meeting in Equity. I have only been to the Equity offices once before, therefore \n",
      "it is not my normal place of work. I can claim my travel, train fare or bus fare to and from the Equity office. \n",
      " \n",
      "\n",
      "4. Regionally/Area based roles \n",
      "\n",
      "If you are regionally/area based, your normal place of work will be your region/area. Any travel within your region/area can be claimed \n",
      "for, unless it is to a normal workplace. If you are regionally based and live outside of your region/area, we have agreed with HMRC \n",
      "that travel to and from the closest site/store in your region/area cannot be claimed for.  (This may not be your normal place of work) \n",
      "This amount of miles must be deducted from any other claims your make when travelling into your region. \n",
      "\n",
      "a. You live within your region \n",
      "\n",
      "If you live inside your region and do not have a normal place of work, any travel to Booker sites can be claimed for. \n",
      "\n",
      "If you live within your region and have a normal place of work/base office within your region, then travel to and from this location \n",
      "cannot be claimed for, as this is ordinary commute. \n",
      "\n",
      "For example: \n",
      "I live within my region, which is Region One.  My normal place of work is Falkirk and I cannot claim my journeys to and from \n",
      "this store as it is my ordinary commute. When I travel to other stores within my region, which are not a normal place of \n",
      "work, I can claim for these journeys. \n",
      "\n",
      "If you live within your region and have a normal place of work somewhere outside of your region, then you will not be able to claim \n",
      "for this journey. \n",
      "\n",
      "For example: \n",
      "I live within my region but attend a team meeting on a fortnightly basis in a set location outside of my region. I can claim \n",
      "for this journey as this would be regarded as a temporary workplace. However, if the team meeting was on a weekly basis, \n",
      "I would be visiting this location on a regular and permanent basis and therefore I cannot claim for the journey as it would \n",
      "be classed as my ordinary commute. \n",
      "\n",
      "b. You live outside your region \n",
      "\n",
      "If you live outside of your region and are travelling into your region to visit a third party or Booker location, we have agreed with HMRC \n",
      "that you cannot claim for any travel from your home to the nearest Booker site within your region and this particular journey will be \n",
      "deemed as your ordinary commute. When you travel to another location within your region, you must deduct this ordinary commute \n",
      "from the mileage you are claiming. \n",
      "\n",
      "5. Nationally based roles \n",
      "\n",
      "If you are nationally based, i.e. your role covers the whole of the UK, all of your travel to third parties and Booker locations (other than \n",
      "to and from a normal place of work e.g. Watford Office) can be claimed. \n",
      "\n",
      "For example: \n",
      "\n",
      "I am nationally based and do not have a normal place of work. On Monday I travel to a store in Leeds, on Tuesday I travel \n",
      "to a store in Scarborough, and on Wednesday I travel to a store in Bristol. I can claim all of my journeys to Booker locations \n",
      "or third parties as none of them have a set pattern or regularity and therefore are not my ordinary commute. \n",
      "\n",
      "I am nationally based and once a week I work from the Northampton branch. This is a normal place of work for me and I \n",
      "cannot claim my journey as it is my ordinary commute. During the rest of the week, I travel to different Booker sites around \n",
      "the country, with no set pattern or regularity. I can claim all of my journeys to these sites as none of them are my ordinary \n",
      "commute.   \n",
      "However if I have more locations that I visit on a weekly basis, I cannot claim for these as they would be classed as normal \n",
      "daily commute. \n",
      "\n",
      "6. Temporary workplace \n",
      "\n",
      "A location is a temporary workplace if you visit there only to perform a task of limited duration, or for a temporary purpose, even if \n",
      "you are visiting it regularly, i.e., weekly, as long as this is for a period of less than 12 months.  This also applies to you if you are on \n",
      "secondment to a different location for less than 24 months, working on a project, covering someone else’s role etc. \n",
      "\n",
      "I work in Hatfield DC but I’m currently working in Hemel Hempstead DC for 10 months in order to work on a project. Even \n",
      "though I am attending the Hemel Hempstead site on a regular basis for 10 months, the project is a task of limited duration \n",
      "and temporary purpose and therefore I can claim my commute from home to Hemel Hempstead, unless it is closer than my \n",
      "normal ordinary  commute. Hatfield would still be classed as my normal place of work. If I visit Hatfield during my time on \n",
      "the project, I cannot claim for this journey. \n",
      "\n",
      "7. When does a temporary workplace become a permanent workplace? \n",
      "\n",
      "Where a temporary location becomes permanent, travel becomes ordinary commute. \n",
      "\n",
      "If you are working at a location for a period of continuous work which lasts, or is likely to last, more than 12 months then this would \n",
      "be classed as a permanent location. A period of continuous work is classed as a period of work where duties of the role are performed \n",
      "to a significant extent at a workplace. HMRC class a ‘significant extent’ at a workplace as 40% or more of your working time spent \n",
      "there. \n",
      "\n",
      "This means that where you have spent or are likely to spend 40% or more of your working time at a particular workplace over a period \n",
      "of 12 months, it will be classed as a permanent workplace. \n",
      "\n",
      "For example: \n",
      "I work in Peterborough but I’m also working on a project in Wellingborough DC on Wednesdays, Thursdays, and Fridays \n",
      "for 18 months. Because I spend more than 40% of my working time at Wellingborough over a period of more than 12 months, \n",
      "Wellingborough is also a  permanent workplace and I cannot claim for my journey as it is my ordinary commute. \n",
      "\n",
      " \n",
      " \n",
      " \n",
      " \n",
      "\n",
      " \n",
      " \n",
      "Guide info \n",
      "\n",
      "Version No. \n",
      "Date of change \n",
      "Summary of change \n",
      "\n",
      "1 \n",
      "February 2023 \n",
      "• \n",
      "New guide \n",
      "\n",
      " \n",
      " \n",
      " \n",
      "\n",
      "Guide owner: \n",
      "Human Resources \n",
      "\n",
      " \n",
      " \n",
      "Ownership and confidentiality \n",
      " \n",
      "\n",
      "This document shouldn’t be shared with anyone externally without permission from your Director. This guide and any \n",
      "\n",
      "associated documentation remains the property of Booker and should be returned if requested. \n",
      "\n",
      "due to other commitments, as in the main it is still your normal place of work. \n",
      "\n",
      "\n",
      "\n",
      "The columns are Commute Type, Description, What is your normal place of work and what\n",
      "can you claim for\n",
      "\n",
      "\n",
      "The columns are Other, I rotate my meetings\n",
      "within a small proximity\n",
      "on a weekly basis, If your meetings are in sites within 10 square\n",
      "miles of each other, they would be classed as an\n",
      "ordinary commute and cannot be claimed for.\n",
      "\n",
      " \n",
      "• \n",
      " Please note that it is the Drivers responsibility to determine that they are using the correct category. \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Image Description Skipped  Ordinary Commute Policy February 2023 What is ordinary commute\n",
      "\n",
      "   3 1  What is classed as a permanent workplace\n",
      "\n",
      "   3 2  Roles with one normal workplace\n",
      "\n",
      "   3 3  Roles with two or more normal workplaces  Dual located or multi sited roles\n",
      "\n",
      "   3 4  Regionally based roles\n",
      "\n",
      "   4 b  You live outside your region\n",
      "\n",
      "   4 5  Nationally based roles\n",
      "\n",
      "   5 6  Temporary workplace\n",
      "\n",
      "   5 7  When does a temporary workplace become a permanent workplace\n",
      "\n",
      "   5 8  FAQ s\n",
      "\n",
      "   7 Guide owner\n",
      "\n",
      "   6 Ownership and confidentiality\n",
      "\n",
      "   6 Appendix 1\n",
      "\n",
      "   Commute type and what you can claim for\n",
      "\n",
      "   8 The commute is based on how often you visit the same location and annual leave is factored into this  You may visit differing people at the same location  or you may visit the same location regularly for only a couple of hours  in both scenario s this would still be classed as an ordinary commute  It is your responsibility to ensure that you comply with the  Commute rules  when completing your normal mileage return  We will conduct regular audits to ensure that there are no unintended infringements  Managers are expected to review their teams mileage to ensure compliance  There is also a requirement for both the Manager and colleague to ensure that the correct normal commute locations are entered into SAP  1  What is classed as a permanent workplace  It is usually clear whether or not a location is your permanent workplace  and  therefore  whether a journey to or from that place is ordinary commuting\n",
      "\n",
      "   A place is a permanent workplace if you attend it regularly to perform your duties of employment  and it is not a temporary workplace  see Section 7\n",
      "\n",
      "   You attend a workplace regularly if your attendance is\n",
      "\n",
      "   Frequent  At least once a week  it does not have to be the same day each week\n",
      "\n",
      "   For all or almost all of the period for which you hold or are likely to hold that employment It s possible that you may have two or more normal workplaces depending on your working arrangements  2  Roles with one normal workplace If you have one normal place of work  travel to and from this location is your ordinary commute and cannot be claimed for  For example  I work in Haydock DC and travel there from home five days a week  I cannot claim any of my journeys to or from Haydock  3  Roles with two or more normal workplaces  Dual located or multi sited roles If you have more than one normal place of work  travel to and from any of these locations is your ordinary commute and cannot be claimed for or supplemented  For example  I am based in the Equity office  but I travel to Eccles every Monday and Deeside every Wednesday  All three of these locations will be my normal place of work and I cannot claim any of my journeys to or from them  If you have one or more normal place s  of work and travel to a different location for a temporary business purpose and a limited duration  travel from home to the different location can be claimed for  Being cost conscious  we don t expect you to claim if your journey is shorter than your ordinary commute and or it costs less  Effectively you are already saving money on your journey  For example  I am based in the Wellingborough DC  but I attend a training day at the Northampton branch  I can claim all of my journey to and from home and the Northampton branch  as this is longer than my normal commute to Wellingborough  When I attended a training day at Kettering  this journey was much shorter than my normal commute to Wellingborough  therefore I didn t claim for the journey  I am based in Hatfield DC and I need to attend a meeting in Equity  I have only been to the Equity offices once before  therefore it is not my normal place of work  I can claim my travel  train fare or bus fare to and from the Equity office  4  Regionally Area based roles If you are regionally area based  your normal place of work will be your region area  Any travel within your region area can be claimed for  unless it is to a normal workplace  If you are regionally based and live outside of your region area  we have agreed with HMRC that travel to and from the closest site store in your region area cannot be claimed for\n",
      "\n",
      "   This may not be your normal place of work  This amount of miles must be deducted from any other claims your make when travelling into your region  a  You live within your region If you live inside your region and do not have a normal place of work  any travel to Booker sites can be claimed for  If you live within your region and have a normal place of work base office within your region  then travel to and from this location cannot be claimed for  as this is ordinary commute  For example  I live within my region  which is Region One  My normal place of work is Falkirk and I cannot claim my journeys to and from this store as it is my ordinary commute  When I travel to other stores within my region  which are not a normal place of work  I can claim for these journeys  If you live within your region and have a normal place of work somewhere outside of your region  then you will not be able to claim for this journey  For example  I live within my region but attend a team meeting on a fortnightly basis in a set location outside of my region  I can claim for this journey as this would be regarded as a temporary workplace  However  if the team meeting was on a weekly basis  I would be visiting this location on a regular and permanent basis and therefore I cannot claim for the journey as it would be classed as my ordinary commute  b  You live outside your region If you live outside of your region and are travelling into your region to visit a third party or Booker location  we have agreed with HMRC that you cannot claim for any travel from your home to the nearest Booker site within your region and this particular journey will be deemed as your ordinary commute  When you travel to another location within your region  you must deduct this ordinary commute from the mileage you are claiming  5  Nationally based roles If you are nationally based  i e  your role covers the whole of the UK  all of your travel to third parties and Booker locations  other than to and from a normal place of work e g  Watford Office  can be claimed  For example  I am nationally based and do not have a normal place of work  On Monday I travel to a store in Leeds  on Tuesday I travel to a store in Scarborough  and on Wednesday I travel to a store in Bristol  I can claim all of my journeys to Booker locations or third parties as none of them have a set pattern or regularity and therefore are not my ordinary commute  I am nationally based and once a week I work from the Northampton branch  This is a normal place of work for me and I cannot claim my journey as it is my ordinary commute  During the rest of the week  I travel to different Booker sites around the country  with no set pattern or regularity  I can claim all of my journeys to these sites as none of them are my ordinary commute  However if I have more locations that I visit on a weekly basis  I cannot claim for these as they would be classed as normal daily commute  6  Temporary workplace A location is a temporary workplace if you visit there only to perform a task of limited duration  or for a temporary purpose  even if you are visiting it regularly  i e\n",
      "\n",
      "   weekly  as long as this is for a period of less than 12 months  This also applies to you if you are on secondment to a different location for less than 24 months  working on a project  covering someone else s role etc  I work in Hatfield DC but I m currently working in Hemel Hempstead DC for 10 months in order to work on a project  Even though I am attending the Hemel Hempstead site on a regular basis for 10 months  the project is a task of limited duration and temporary purpose and therefore I can claim my commute from home to Hemel Hempstead  unless it is closer than my normal ordinary commute  Hatfield would still be classed as my normal place of work  If I visit Hatfield during my time on the project  I cannot claim for this journey  7  When does a temporary workplace become a permanent workplace  Where a temporary location becomes permanent  travel becomes ordinary commute  If you are working at a location for a period of continuous work which lasts  or is likely to last  more than 12 months then this would be classed as a permanent location  A period of continuous work is classed as a period of work where duties of the role are performed to a significant extent at a workplace  HMRC class a  significant extent  at a workplace as 40  or more of your working time spent there  This means that where you have spent or are likely to spend 40  or more of your working time at a particular workplace over a period of 12 months  it will be classed as a permanent workplace  For example  I work in Peterborough but I m also working on a project in Wellingborough DC on Wednesdays  Thursdays  and Fridays for 18 months  Because I spend more than 40  of my working time at Wellingborough over a period of more than 12 months  Wellingborough is also a permanent workplace and I cannot claim for my journey as it is my ordinary commute  Guide info Version No  Date of change Summary of change 1 February 2023\n",
      "\n",
      "   New guide Guide owner  Human Resources Ownership and confidentiality This document shouldn t be shared with anyone externally without permission from your Director  This guide and any associated documentation remains the property of Booker and should be returned if requested  due to other commitments  as in the main it is still your normal place of work  The columns are Commute Type  Description  What is your normal place of work and what can you claim for The columns are Other  I rotate my meetings within a small proximity on a weekly basis  If your meetings are in sites within 10 square miles of each other  they would be classed as an ordinary commute and cannot be claimed for\n",
      "\n",
      "   Please note that it is the Drivers responsibility to determine that they are using the correct category\n",
      "\n",
      "\n",
      "###################   Genrating raw chunks   ###################\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'type': 'error', 'error': {'type': 'authentication_error', 'message': 'invalid x-api-key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 17\u001b[0m\n\u001b[0;32m     15\u001b[0m raw_chunk \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m segment \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[1;32m---> 17\u001b[0m     raw_chunk_ \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_raw_chunks\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msegment\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m     raw_chunk \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(raw_chunk_)\n\u001b[0;32m     19\u001b[0m log(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw Chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[8], line 31\u001b[0m, in \u001b[0;36mgenerate_raw_chunks\u001b[1;34m(user_prompt)\u001b[0m\n\u001b[0;32m      6\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mGiven the provided text data, your task is to chunk the text into meaningful segments or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mchunks\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m based on the topics or sections mentioned within the text. Each chunk should encapsulate a distinct topic or subtopic discussed within the text corpus. Your goal is to parse the text into coherent units that represent the main themes or ideas conveyed in the text.\u001b[39m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;124mYou can identify the boundaries of each chunk by looking for section headers or topic labels within the text. These headers typically indicate the start of a new topic or section. Your output should consist of the identified chunks, along with their corresponding labels or headers.\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     29\u001b[0m log(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenrating raw chunks\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 31\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclaude-3-opus-20240229\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\n\u001b[0;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHere is the corpse\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m <important> You are strictly not allowed to modify this corpse your only job is to split this corpse into chunks(that makes sense)</important>\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m<corpse>\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43muser_prompt\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m</corpse>\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     39\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m , response)\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mcontent[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\n",
      "File \u001b[1;32me:\\Projects\\SA - R&D\\chunking\\venv\\Lib\\site-packages\\anthropic\\_utils\\_utils.py:277\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    275\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    276\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[1;32m--> 277\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Projects\\SA - R&D\\chunking\\venv\\Lib\\site-packages\\anthropic\\resources\\messages.py:678\u001b[0m, in \u001b[0;36mMessages.create\u001b[1;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[0;32m    647\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    648\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m600\u001b[39m,\n\u001b[0;32m    677\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Message \u001b[38;5;241m|\u001b[39m Stream[MessageStreamEvent]:\n\u001b[1;32m--> 678\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    679\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/v1/messages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    680\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    681\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    683\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    684\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    685\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    686\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop_sequences\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    687\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    688\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    689\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    690\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_k\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    691\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    692\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    693\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessage_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMessageCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    694\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    696\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[0;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mMessage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mMessageStreamEvent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Projects\\SA - R&D\\chunking\\venv\\Lib\\site-packages\\anthropic\\_base_client.py:1232\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1218\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1220\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1227\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1228\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m   1229\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[0;32m   1230\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[0;32m   1231\u001b[0m     )\n\u001b[1;32m-> 1232\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32me:\\Projects\\SA - R&D\\chunking\\venv\\Lib\\site-packages\\anthropic\\_base_client.py:921\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m    912\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    914\u001b[0m     cast_to: Type[ResponseT],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    919\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    920\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    922\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    923\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    924\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    925\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    926\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Projects\\SA - R&D\\chunking\\venv\\Lib\\site-packages\\anthropic\\_base_client.py:1012\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[0;32m   1009\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m   1011\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1012\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1014\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[0;32m   1015\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[0;32m   1016\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1019\u001b[0m     stream_cls\u001b[38;5;241m=\u001b[39mstream_cls,\n\u001b[0;32m   1020\u001b[0m )\n",
      "\u001b[1;31mAuthenticationError\u001b[0m: Error code: 401 - {'type': 'error', 'error': {'type': 'authentication_error', 'message': 'invalid x-api-key'}}"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    import time \n",
    "    \n",
    "    start_time = time.time()\n",
    "    log(\"Called PDF extracter\")\n",
    "    # corpus = extract_data(pdf_path_or_url=r\"./data\\mlpdf.pdf\")\n",
    "    corpus = main(r\"E:\\Projects\\SA - R&D\\chunking\\resources\\data\\Companycar.pdf\")\n",
    "    log(\"Extracted PDF data\")\n",
    "    print(corpus)\n",
    "    corpus = format_text(corpus)\n",
    "    print(\"\\n\\n\")\n",
    "    print(corpus)\n",
    "    result = split_corpse(corpus)\n",
    "    raw_chunk = ''\n",
    "    for segment in result:\n",
    "        raw_chunk_ = generate_raw_chunks(user_prompt=segment)\n",
    "        raw_chunk + \"\\n\\n\".join(raw_chunk_)\n",
    "    log(\"Raw Chunks\")\n",
    "    print(raw_chunk)\n",
    "    pre_process(corpus=corpus,raw_chunks=raw_chunk,test_flag=False,save_flag=True,display_flag=False)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # Calculate the total time taken\n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    log(f\" Total time taken to run: {total_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Shared Parental Leave Flowchart.pdf'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.path.basename(r\"E:\\Projects\\SA - R&D\\chunking\\data\\Shared Parental Leave Flowchart.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
